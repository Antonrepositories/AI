{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5765f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ab3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (22899, 20)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22899 entries, 0 to 22898\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   latitude                        22899 non-null  float64\n",
      " 1   longitude                       22899 non-null  float64\n",
      " 2   mag                             22899 non-null  float64\n",
      " 3   clustering_coefficient_30_days  22899 non-null  float64\n",
      " 4   std_mag_30_days                 22899 non-null  float64\n",
      " 5   rolling_mean_depth_30_days      22899 non-null  float64\n",
      " 6   earthquakes_last_30_days        22899 non-null  int64  \n",
      " 7   b_value                         22899 non-null  float64\n",
      " 8   b_value_increment_i_i2          22899 non-null  float64\n",
      " 9   b_value_increment_i2_i4         22899 non-null  float64\n",
      " 10  b_value_increment_i4_i6         22899 non-null  float64\n",
      " 11  b_value_increment_i6_i8         22899 non-null  float64\n",
      " 12  b_value_increment_i8_i10        22899 non-null  float64\n",
      " 13  max_mag_last_week               22899 non-null  float64\n",
      " 14  eta                             22899 non-null  float64\n",
      " 15  delta_M                         22899 non-null  float64\n",
      " 16  elapsed_time                    22899 non-null  float64\n",
      " 17  coefficient_of_variation        22899 non-null  float64\n",
      " 18  dE1_2                           22899 non-null  float64\n",
      " 19  class                           22899 non-null  int64  \n",
      "dtypes: float64(18), int64(2)\n",
      "memory usage: 3.5 MB\n",
      "None\n",
      "    latitude   longitude   mag  clustering_coefficient_30_days  \\\n",
      "0  33.736167 -117.543667  1.29                        0.761985   \n",
      "1  34.231167 -117.613333  1.24                        0.763917   \n",
      "2  34.448333 -119.028667  2.77                        0.759888   \n",
      "3  33.987667 -117.246500  0.92                        0.763414   \n",
      "4  34.453500 -117.954333  0.82                        0.760808   \n",
      "\n",
      "   std_mag_30_days  rolling_mean_depth_30_days  earthquakes_last_30_days  \\\n",
      "0         0.450064                   10.010000                        62   \n",
      "1         0.447643                    9.956730                        63   \n",
      "2         0.471666                    9.982750                        64   \n",
      "3         0.473813                   10.037185                        65   \n",
      "4         0.477744                   10.007470                        66   \n",
      "\n",
      "    b_value  b_value_increment_i_i2  b_value_increment_i2_i4  \\\n",
      "0  0.696878               -0.015781                -0.016952   \n",
      "1  0.709632                0.005751                -0.005374   \n",
      "2  0.702969                0.006091                -0.009690   \n",
      "3  0.679434               -0.030197                -0.024446   \n",
      "4  0.721180                0.018210                 0.024302   \n",
      "\n",
      "   b_value_increment_i4_i6  b_value_increment_i6_i8  b_value_increment_i8_i10  \\\n",
      "0                 0.027703                 0.045957                  0.068377   \n",
      "1                 0.022239                 0.063360                  0.081494   \n",
      "2                -0.010861                 0.033795                  0.052048   \n",
      "3                -0.035571                -0.007958                  0.033163   \n",
      "4                 0.008521                 0.007349                  0.052005   \n",
      "\n",
      "   max_mag_last_week       eta   delta_M  elapsed_time  \\\n",
      "0               1.70  0.171183 -1.191173  1.826028e+06   \n",
      "1               1.70  0.171273 -1.136158  1.719504e+06   \n",
      "2               1.70  0.171953 -1.164648  1.626713e+06   \n",
      "3               2.77  0.163891 -1.269765  1.643494e+06   \n",
      "4               2.77  0.170929 -1.088020  1.686714e+06   \n",
      "\n",
      "   coefficient_of_variation         dE1_2  class  \n",
      "0                  1.030479  60072.253071      3  \n",
      "1                  1.018022  59804.049628      3  \n",
      "2                  1.040813  59828.224358      3  \n",
      "3                  1.032048  66772.751598      3  \n",
      "4                  1.045357  59653.522777      3  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"LosAngeles_Earthquake_Dataset.csv\")\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d64348df",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'mag',\n",
    "    'clustering_coefficient_30_days',\n",
    "    'std_mag_30_days',\n",
    "    'rolling_mean_depth_30_days',\n",
    "    'earthquakes_last_30_days',\n",
    "    'b_value',\n",
    "    'b_value_increment_i_i2',\n",
    "    'b_value_increment_i2_i4',\n",
    "    'b_value_increment_i4_i6',\n",
    "    'b_value_increment_i6_i8',\n",
    "    'b_value_increment_i8_i10',\n",
    "    'max_mag_last_week',\n",
    "    'eta',\n",
    "    'delta_M',\n",
    "    'elapsed_time',\n",
    "    'coefficient_of_variation',\n",
    "    'dE1_2'\n",
    "]\n",
    "target_col = 'class'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f88adef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[feature_cols + [target_col]].dropna()\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df[target_col])\n",
    "\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b22ecbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "291c27a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aea88558",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthquakeNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y))\n",
    "model = EarthquakeNN(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6162c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/3500], Loss: 1.7196\n",
      "Epoch [20/3500], Loss: 1.6600\n",
      "Epoch [30/3500], Loss: 1.6156\n",
      "Epoch [40/3500], Loss: 1.5856\n",
      "Epoch [50/3500], Loss: 1.5674\n",
      "Epoch [60/3500], Loss: 1.5530\n",
      "Epoch [70/3500], Loss: 1.5392\n",
      "Epoch [80/3500], Loss: 1.5252\n",
      "Epoch [90/3500], Loss: 1.5105\n",
      "Epoch [100/3500], Loss: 1.4951\n",
      "Epoch [110/3500], Loss: 1.4790\n",
      "Epoch [120/3500], Loss: 1.4624\n",
      "Epoch [130/3500], Loss: 1.4450\n",
      "Epoch [140/3500], Loss: 1.4270\n",
      "Epoch [150/3500], Loss: 1.4081\n",
      "Epoch [160/3500], Loss: 1.3886\n",
      "Epoch [170/3500], Loss: 1.3687\n",
      "Epoch [180/3500], Loss: 1.3481\n",
      "Epoch [190/3500], Loss: 1.3274\n",
      "Epoch [200/3500], Loss: 1.3069\n",
      "Epoch [210/3500], Loss: 1.2867\n",
      "Epoch [220/3500], Loss: 1.2667\n",
      "Epoch [230/3500], Loss: 1.2470\n",
      "Epoch [240/3500], Loss: 1.2278\n",
      "Epoch [250/3500], Loss: 1.2087\n",
      "Epoch [260/3500], Loss: 1.1897\n",
      "Epoch [270/3500], Loss: 1.1710\n",
      "Epoch [280/3500], Loss: 1.1523\n",
      "Epoch [290/3500], Loss: 1.1338\n",
      "Epoch [300/3500], Loss: 1.1154\n",
      "Epoch [310/3500], Loss: 1.0971\n",
      "Epoch [320/3500], Loss: 1.0788\n",
      "Epoch [330/3500], Loss: 1.0603\n",
      "Epoch [340/3500], Loss: 1.0417\n",
      "Epoch [350/3500], Loss: 1.0233\n",
      "Epoch [360/3500], Loss: 1.0045\n",
      "Epoch [370/3500], Loss: 0.9856\n",
      "Epoch [380/3500], Loss: 0.9668\n",
      "Epoch [390/3500], Loss: 0.9483\n",
      "Epoch [400/3500], Loss: 0.9298\n",
      "Epoch [410/3500], Loss: 0.9111\n",
      "Epoch [420/3500], Loss: 0.8927\n",
      "Epoch [430/3500], Loss: 0.8749\n",
      "Epoch [440/3500], Loss: 0.8572\n",
      "Epoch [450/3500], Loss: 0.8398\n",
      "Epoch [460/3500], Loss: 0.8232\n",
      "Epoch [470/3500], Loss: 0.8070\n",
      "Epoch [480/3500], Loss: 0.7917\n",
      "Epoch [490/3500], Loss: 0.7766\n",
      "Epoch [500/3500], Loss: 0.7618\n",
      "Epoch [510/3500], Loss: 0.7476\n",
      "Epoch [520/3500], Loss: 0.7333\n",
      "Epoch [530/3500], Loss: 0.7196\n",
      "Epoch [540/3500], Loss: 0.7061\n",
      "Epoch [550/3500], Loss: 0.6928\n",
      "Epoch [560/3500], Loss: 0.6800\n",
      "Epoch [570/3500], Loss: 0.6675\n",
      "Epoch [580/3500], Loss: 0.6554\n",
      "Epoch [590/3500], Loss: 0.6436\n",
      "Epoch [600/3500], Loss: 0.6324\n",
      "Epoch [610/3500], Loss: 0.6215\n",
      "Epoch [620/3500], Loss: 0.6113\n",
      "Epoch [630/3500], Loss: 0.6012\n",
      "Epoch [640/3500], Loss: 0.5916\n",
      "Epoch [650/3500], Loss: 0.5821\n",
      "Epoch [660/3500], Loss: 0.5729\n",
      "Epoch [670/3500], Loss: 0.5640\n",
      "Epoch [680/3500], Loss: 0.5549\n",
      "Epoch [690/3500], Loss: 0.5461\n",
      "Epoch [700/3500], Loss: 0.5377\n",
      "Epoch [710/3500], Loss: 0.5296\n",
      "Epoch [720/3500], Loss: 0.5215\n",
      "Epoch [730/3500], Loss: 0.5137\n",
      "Epoch [740/3500], Loss: 0.5062\n",
      "Epoch [750/3500], Loss: 0.4989\n",
      "Epoch [760/3500], Loss: 0.4914\n",
      "Epoch [770/3500], Loss: 0.4846\n",
      "Epoch [780/3500], Loss: 0.4779\n",
      "Epoch [790/3500], Loss: 0.4717\n",
      "Epoch [800/3500], Loss: 0.4651\n",
      "Epoch [810/3500], Loss: 0.4588\n",
      "Epoch [820/3500], Loss: 0.4526\n",
      "Epoch [830/3500], Loss: 0.4466\n",
      "Epoch [840/3500], Loss: 0.4412\n",
      "Epoch [850/3500], Loss: 0.4351\n",
      "Epoch [860/3500], Loss: 0.4293\n",
      "Epoch [870/3500], Loss: 0.4238\n",
      "Epoch [880/3500], Loss: 0.4183\n",
      "Epoch [890/3500], Loss: 0.4129\n",
      "Epoch [900/3500], Loss: 0.4077\n",
      "Epoch [910/3500], Loss: 0.4024\n",
      "Epoch [920/3500], Loss: 0.3973\n",
      "Epoch [930/3500], Loss: 0.3927\n",
      "Epoch [940/3500], Loss: 0.3875\n",
      "Epoch [950/3500], Loss: 0.3827\n",
      "Epoch [960/3500], Loss: 0.3780\n",
      "Epoch [970/3500], Loss: 0.3734\n",
      "Epoch [980/3500], Loss: 0.3689\n",
      "Epoch [990/3500], Loss: 0.3646\n",
      "Epoch [1000/3500], Loss: 0.3604\n",
      "Epoch [1010/3500], Loss: 0.3563\n",
      "Epoch [1020/3500], Loss: 0.3521\n",
      "Epoch [1030/3500], Loss: 0.3481\n",
      "Epoch [1040/3500], Loss: 0.3445\n",
      "Epoch [1050/3500], Loss: 0.3404\n",
      "Epoch [1060/3500], Loss: 0.3366\n",
      "Epoch [1070/3500], Loss: 0.3329\n",
      "Epoch [1080/3500], Loss: 0.3291\n",
      "Epoch [1090/3500], Loss: 0.3255\n",
      "Epoch [1100/3500], Loss: 0.3218\n",
      "Epoch [1110/3500], Loss: 0.3184\n",
      "Epoch [1120/3500], Loss: 0.3145\n",
      "Epoch [1130/3500], Loss: 0.3110\n",
      "Epoch [1140/3500], Loss: 0.3076\n",
      "Epoch [1150/3500], Loss: 0.3042\n",
      "Epoch [1160/3500], Loss: 0.3013\n",
      "Epoch [1170/3500], Loss: 0.2978\n",
      "Epoch [1180/3500], Loss: 0.2945\n",
      "Epoch [1190/3500], Loss: 0.2914\n",
      "Epoch [1200/3500], Loss: 0.2883\n",
      "Epoch [1210/3500], Loss: 0.2851\n",
      "Epoch [1220/3500], Loss: 0.2823\n",
      "Epoch [1230/3500], Loss: 0.2792\n",
      "Epoch [1240/3500], Loss: 0.2762\n",
      "Epoch [1250/3500], Loss: 0.2733\n",
      "Epoch [1260/3500], Loss: 0.2707\n",
      "Epoch [1270/3500], Loss: 0.2677\n",
      "Epoch [1280/3500], Loss: 0.2649\n",
      "Epoch [1290/3500], Loss: 0.2623\n",
      "Epoch [1300/3500], Loss: 0.2596\n",
      "Epoch [1310/3500], Loss: 0.2570\n",
      "Epoch [1320/3500], Loss: 0.2546\n",
      "Epoch [1330/3500], Loss: 0.2521\n",
      "Epoch [1340/3500], Loss: 0.2494\n",
      "Epoch [1350/3500], Loss: 0.2469\n",
      "Epoch [1360/3500], Loss: 0.2445\n",
      "Epoch [1370/3500], Loss: 0.2422\n",
      "Epoch [1380/3500], Loss: 0.2398\n",
      "Epoch [1390/3500], Loss: 0.2380\n",
      "Epoch [1400/3500], Loss: 0.2355\n",
      "Epoch [1410/3500], Loss: 0.2332\n",
      "Epoch [1420/3500], Loss: 0.2309\n",
      "Epoch [1430/3500], Loss: 0.2289\n",
      "Epoch [1440/3500], Loss: 0.2267\n",
      "Epoch [1450/3500], Loss: 0.2246\n",
      "Epoch [1460/3500], Loss: 0.2224\n",
      "Epoch [1470/3500], Loss: 0.2203\n",
      "Epoch [1480/3500], Loss: 0.2183\n",
      "Epoch [1490/3500], Loss: 0.2162\n",
      "Epoch [1500/3500], Loss: 0.2147\n",
      "Epoch [1510/3500], Loss: 0.2125\n",
      "Epoch [1520/3500], Loss: 0.2106\n",
      "Epoch [1530/3500], Loss: 0.2085\n",
      "Epoch [1540/3500], Loss: 0.2069\n",
      "Epoch [1550/3500], Loss: 0.2049\n",
      "Epoch [1560/3500], Loss: 0.2029\n",
      "Epoch [1570/3500], Loss: 0.2011\n",
      "Epoch [1580/3500], Loss: 0.1993\n",
      "Epoch [1590/3500], Loss: 0.1976\n",
      "Epoch [1600/3500], Loss: 0.1958\n",
      "Epoch [1610/3500], Loss: 0.1939\n",
      "Epoch [1620/3500], Loss: 0.1922\n",
      "Epoch [1630/3500], Loss: 0.1904\n",
      "Epoch [1640/3500], Loss: 0.1889\n",
      "Epoch [1650/3500], Loss: 0.1872\n",
      "Epoch [1660/3500], Loss: 0.1856\n",
      "Epoch [1670/3500], Loss: 0.1840\n",
      "Epoch [1680/3500], Loss: 0.1823\n",
      "Epoch [1690/3500], Loss: 0.1808\n",
      "Epoch [1700/3500], Loss: 0.1794\n",
      "Epoch [1710/3500], Loss: 0.1778\n",
      "Epoch [1720/3500], Loss: 0.1761\n",
      "Epoch [1730/3500], Loss: 0.1747\n",
      "Epoch [1740/3500], Loss: 0.1732\n",
      "Epoch [1750/3500], Loss: 0.1717\n",
      "Epoch [1760/3500], Loss: 0.1703\n",
      "Epoch [1770/3500], Loss: 0.1688\n",
      "Epoch [1780/3500], Loss: 0.1674\n",
      "Epoch [1790/3500], Loss: 0.1661\n",
      "Epoch [1800/3500], Loss: 0.1646\n",
      "Epoch [1810/3500], Loss: 0.1632\n",
      "Epoch [1820/3500], Loss: 0.1618\n",
      "Epoch [1830/3500], Loss: 0.1605\n",
      "Epoch [1840/3500], Loss: 0.1593\n",
      "Epoch [1850/3500], Loss: 0.1578\n",
      "Epoch [1860/3500], Loss: 0.1566\n",
      "Epoch [1870/3500], Loss: 0.1554\n",
      "Epoch [1880/3500], Loss: 0.1540\n",
      "Epoch [1890/3500], Loss: 0.1528\n",
      "Epoch [1900/3500], Loss: 0.1516\n",
      "Epoch [1910/3500], Loss: 0.1504\n",
      "Epoch [1920/3500], Loss: 0.1491\n",
      "Epoch [1930/3500], Loss: 0.1479\n",
      "Epoch [1940/3500], Loss: 0.1469\n",
      "Epoch [1950/3500], Loss: 0.1456\n",
      "Epoch [1960/3500], Loss: 0.1448\n",
      "Epoch [1970/3500], Loss: 0.1434\n",
      "Epoch [1980/3500], Loss: 0.1422\n",
      "Epoch [1990/3500], Loss: 0.1411\n",
      "Epoch [2000/3500], Loss: 0.1400\n",
      "Epoch [2010/3500], Loss: 0.1390\n",
      "Epoch [2020/3500], Loss: 0.1380\n",
      "Epoch [2030/3500], Loss: 0.1368\n",
      "Epoch [2040/3500], Loss: 0.1358\n",
      "Epoch [2050/3500], Loss: 0.1349\n",
      "Epoch [2060/3500], Loss: 0.1338\n",
      "Epoch [2070/3500], Loss: 0.1329\n",
      "Epoch [2080/3500], Loss: 0.1318\n",
      "Epoch [2090/3500], Loss: 0.1307\n",
      "Epoch [2100/3500], Loss: 0.1298\n",
      "Epoch [2110/3500], Loss: 0.1288\n",
      "Epoch [2120/3500], Loss: 0.1280\n",
      "Epoch [2130/3500], Loss: 0.1269\n",
      "Epoch [2140/3500], Loss: 0.1260\n",
      "Epoch [2150/3500], Loss: 0.1250\n",
      "Epoch [2160/3500], Loss: 0.1245\n",
      "Epoch [2170/3500], Loss: 0.1234\n",
      "Epoch [2180/3500], Loss: 0.1224\n",
      "Epoch [2190/3500], Loss: 0.1215\n",
      "Epoch [2200/3500], Loss: 0.1206\n",
      "Epoch [2210/3500], Loss: 0.1198\n",
      "Epoch [2220/3500], Loss: 0.1189\n",
      "Epoch [2230/3500], Loss: 0.1181\n",
      "Epoch [2240/3500], Loss: 0.1172\n",
      "Epoch [2250/3500], Loss: 0.1166\n",
      "Epoch [2260/3500], Loss: 0.1157\n",
      "Epoch [2270/3500], Loss: 0.1149\n",
      "Epoch [2280/3500], Loss: 0.1140\n",
      "Epoch [2290/3500], Loss: 0.1131\n",
      "Epoch [2300/3500], Loss: 0.1126\n",
      "Epoch [2310/3500], Loss: 0.1116\n",
      "Epoch [2320/3500], Loss: 0.1108\n",
      "Epoch [2330/3500], Loss: 0.1100\n",
      "Epoch [2340/3500], Loss: 0.1093\n",
      "Epoch [2350/3500], Loss: 0.1087\n",
      "Epoch [2360/3500], Loss: 0.1078\n",
      "Epoch [2370/3500], Loss: 0.1071\n",
      "Epoch [2380/3500], Loss: 0.1065\n",
      "Epoch [2390/3500], Loss: 0.1057\n",
      "Epoch [2400/3500], Loss: 0.1050\n",
      "Epoch [2410/3500], Loss: 0.1042\n",
      "Epoch [2420/3500], Loss: 0.1036\n",
      "Epoch [2430/3500], Loss: 0.1030\n",
      "Epoch [2440/3500], Loss: 0.1021\n",
      "Epoch [2450/3500], Loss: 0.1015\n",
      "Epoch [2460/3500], Loss: 0.1008\n",
      "Epoch [2470/3500], Loss: 0.1000\n",
      "Epoch [2480/3500], Loss: 0.0994\n",
      "Epoch [2490/3500], Loss: 0.0988\n",
      "Epoch [2500/3500], Loss: 0.0980\n",
      "Epoch [2510/3500], Loss: 0.0974\n",
      "Epoch [2520/3500], Loss: 0.0968\n",
      "Epoch [2530/3500], Loss: 0.0960\n",
      "Epoch [2540/3500], Loss: 0.0954\n",
      "Epoch [2550/3500], Loss: 0.0948\n",
      "Epoch [2560/3500], Loss: 0.0943\n",
      "Epoch [2570/3500], Loss: 0.0937\n",
      "Epoch [2580/3500], Loss: 0.0930\n",
      "Epoch [2590/3500], Loss: 0.0924\n",
      "Epoch [2600/3500], Loss: 0.0918\n",
      "Epoch [2610/3500], Loss: 0.0913\n",
      "Epoch [2620/3500], Loss: 0.0906\n",
      "Epoch [2630/3500], Loss: 0.0901\n",
      "Epoch [2640/3500], Loss: 0.0895\n",
      "Epoch [2650/3500], Loss: 0.0891\n",
      "Epoch [2660/3500], Loss: 0.0884\n",
      "Epoch [2670/3500], Loss: 0.0878\n",
      "Epoch [2680/3500], Loss: 0.0874\n",
      "Epoch [2690/3500], Loss: 0.0867\n",
      "Epoch [2700/3500], Loss: 0.0863\n",
      "Epoch [2710/3500], Loss: 0.0857\n",
      "Epoch [2720/3500], Loss: 0.0851\n",
      "Epoch [2730/3500], Loss: 0.0847\n",
      "Epoch [2740/3500], Loss: 0.0842\n",
      "Epoch [2750/3500], Loss: 0.0836\n",
      "Epoch [2760/3500], Loss: 0.0831\n",
      "Epoch [2770/3500], Loss: 0.0826\n",
      "Epoch [2780/3500], Loss: 0.0822\n",
      "Epoch [2790/3500], Loss: 0.0817\n",
      "Epoch [2800/3500], Loss: 0.0811\n",
      "Epoch [2810/3500], Loss: 0.0807\n",
      "Epoch [2820/3500], Loss: 0.0802\n",
      "Epoch [2830/3500], Loss: 0.0797\n",
      "Epoch [2840/3500], Loss: 0.0793\n",
      "Epoch [2850/3500], Loss: 0.0787\n",
      "Epoch [2860/3500], Loss: 0.0784\n",
      "Epoch [2870/3500], Loss: 0.0778\n",
      "Epoch [2880/3500], Loss: 0.0774\n",
      "Epoch [2890/3500], Loss: 0.0769\n",
      "Epoch [2900/3500], Loss: 0.0765\n",
      "Epoch [2910/3500], Loss: 0.0760\n",
      "Epoch [2920/3500], Loss: 0.0755\n",
      "Epoch [2930/3500], Loss: 0.0751\n",
      "Epoch [2940/3500], Loss: 0.0747\n",
      "Epoch [2950/3500], Loss: 0.0741\n",
      "Epoch [2960/3500], Loss: 0.0738\n",
      "Epoch [2970/3500], Loss: 0.0734\n",
      "Epoch [2980/3500], Loss: 0.0730\n",
      "Epoch [2990/3500], Loss: 0.0726\n",
      "Epoch [3000/3500], Loss: 0.0721\n",
      "Epoch [3010/3500], Loss: 0.0716\n",
      "Epoch [3020/3500], Loss: 0.0713\n",
      "Epoch [3030/3500], Loss: 0.0708\n",
      "Epoch [3040/3500], Loss: 0.0704\n",
      "Epoch [3050/3500], Loss: 0.0701\n",
      "Epoch [3060/3500], Loss: 0.0697\n",
      "Epoch [3070/3500], Loss: 0.0693\n",
      "Epoch [3080/3500], Loss: 0.0690\n",
      "Epoch [3090/3500], Loss: 0.0684\n",
      "Epoch [3100/3500], Loss: 0.0681\n",
      "Epoch [3110/3500], Loss: 0.0677\n",
      "Epoch [3120/3500], Loss: 0.0673\n",
      "Epoch [3130/3500], Loss: 0.0669\n",
      "Epoch [3140/3500], Loss: 0.0666\n",
      "Epoch [3150/3500], Loss: 0.0662\n",
      "Epoch [3160/3500], Loss: 0.0659\n",
      "Epoch [3170/3500], Loss: 0.0654\n",
      "Epoch [3180/3500], Loss: 0.0652\n",
      "Epoch [3190/3500], Loss: 0.0647\n",
      "Epoch [3200/3500], Loss: 0.0644\n",
      "Epoch [3210/3500], Loss: 0.0640\n",
      "Epoch [3220/3500], Loss: 0.0637\n",
      "Epoch [3230/3500], Loss: 0.0633\n",
      "Epoch [3240/3500], Loss: 0.0630\n",
      "Epoch [3250/3500], Loss: 0.0626\n",
      "Epoch [3260/3500], Loss: 0.0622\n",
      "Epoch [3270/3500], Loss: 0.0621\n",
      "Epoch [3280/3500], Loss: 0.0616\n",
      "Epoch [3290/3500], Loss: 0.0612\n",
      "Epoch [3300/3500], Loss: 0.0609\n",
      "Epoch [3310/3500], Loss: 0.0605\n",
      "Epoch [3320/3500], Loss: 0.0603\n",
      "Epoch [3330/3500], Loss: 0.0599\n",
      "Epoch [3340/3500], Loss: 0.0595\n",
      "Epoch [3350/3500], Loss: 0.0592\n",
      "Epoch [3360/3500], Loss: 0.0588\n",
      "Epoch [3370/3500], Loss: 0.0586\n",
      "Epoch [3380/3500], Loss: 0.0582\n",
      "Epoch [3390/3500], Loss: 0.0578\n",
      "Epoch [3400/3500], Loss: 0.0575\n",
      "Epoch [3410/3500], Loss: 0.0573\n",
      "Epoch [3420/3500], Loss: 0.0569\n",
      "Epoch [3430/3500], Loss: 0.0566\n",
      "Epoch [3440/3500], Loss: 0.0563\n",
      "Epoch [3450/3500], Loss: 0.0560\n",
      "Epoch [3460/3500], Loss: 0.0557\n",
      "Epoch [3470/3500], Loss: 0.0554\n",
      "Epoch [3480/3500], Loss: 0.0550\n",
      "Epoch [3490/3500], Loss: 0.0550\n",
      "Epoch [3500/3500], Loss: 0.0545\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "epochs = 3500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_t)\n",
    "    loss = criterion(outputs, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "861e395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 0.9150655021834061\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.91      0.91       942\n",
      "           2       0.92      0.92      0.92      1269\n",
      "           3       0.93      0.91      0.92      1040\n",
      "           4       0.93      0.93      0.93       687\n",
      "           5       0.89      0.90      0.89       527\n",
      "           6       0.94      0.90      0.92       115\n",
      "\n",
      "    accuracy                           0.92      4580\n",
      "   macro avg       0.92      0.91      0.91      4580\n",
      "weighted avg       0.92      0.92      0.92      4580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t)\n",
    "    predicted = torch.argmax(preds, dim=1).numpy()\n",
    "target_names1 = [str(c) for c in label_encoder.classes_]\n",
    "\n",
    "acc = accuracy_score(y_test, predicted)\n",
    "print(\"\\nAccuracy:\", acc)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, predicted, target_names=target_names1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2aa90437",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, lr=0.001, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        # Xavier initialization\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2 / (input_dim + hidden_dim))\n",
    "        self.b1 = np.zeros((1, hidden_dim))\n",
    "        self.W2 = np.random.randn(hidden_dim, hidden_dim) * np.sqrt(2 / (hidden_dim * 2))\n",
    "        self.b2 = np.zeros((1, hidden_dim))\n",
    "        self.W3 = np.random.randn(hidden_dim, output_dim) * np.sqrt(2 / (hidden_dim + output_dim))\n",
    "        self.b3 = np.zeros((1, output_dim))\n",
    "        self.lr = lr\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def drelu(self, x):\n",
    "        return (x > 0).astype(float)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        a2 = self.relu(z2)\n",
    "        z3 = a2 @ self.W3 + self.b3\n",
    "        a3 = self.softmax(z3)\n",
    "        return (z1, a1, z2, a2, z3, a3)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        # Cross-entropy\n",
    "        n = y_true.shape[0]\n",
    "        log_probs = -np.log(y_pred[range(n), y_true] + 1e-8)\n",
    "        return np.mean(log_probs)\n",
    "\n",
    "    def backward(self, X, y_true, cache):\n",
    "        z1, a1, z2, a2, z3, a3 = cache\n",
    "        n = X.shape[0]\n",
    "\n",
    "        # one-hot\n",
    "        y_onehot = np.zeros_like(a3)\n",
    "        y_onehot[np.arange(n), y_true] = 1\n",
    "\n",
    "        dz3 = (a3 - y_onehot) / n\n",
    "        dW3 = a2.T @ dz3\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True)\n",
    "\n",
    "        da2 = dz3 @ self.W3.T\n",
    "        dz2 = da2 * self.drelu(z2)\n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.drelu(z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "        # gradient descent step\n",
    "        self.W1 -= self.lr * dW1\n",
    "        self.b1 -= self.lr * db1\n",
    "        self.W2 -= self.lr * dW2\n",
    "        self.b2 -= self.lr * db2\n",
    "        self.W3 -= self.lr * dW3\n",
    "        self.b3 -= self.lr * db3\n",
    "\n",
    "    def train(self, X, y, epochs=100, batch_size=32):\n",
    "        n = X.shape[0]\n",
    "        losses = []\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            idx = np.random.permutation(n)\n",
    "            X = X[idx]\n",
    "            y = y[idx]\n",
    "            batch_loss = 0\n",
    "            for i in range(0, n, batch_size):\n",
    "                xb = X[i:i + batch_size]\n",
    "                yb = y[i:i + batch_size]\n",
    "                cache = self.forward(xb)\n",
    "                loss = self.compute_loss(yb, cache[-1])\n",
    "                batch_loss += loss * xb.shape[0]\n",
    "                self.backward(xb, yb, cache)\n",
    "            epoch_loss = batch_loss / n\n",
    "            losses.append(epoch_loss)\n",
    "            if epoch % 10 == 0 or epoch == 1:\n",
    "                print(f\"[NumPy NN] Epoch {epoch}/{epochs} - loss: {epoch_loss:.4f}\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, _, _, _, _, a3 = self.forward(X)\n",
    "        return np.argmax(a3, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d12cf3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumPy NN] Epoch 1/3500 - loss: 1.8065\n",
      "[NumPy NN] Epoch 10/3500 - loss: 1.5996\n",
      "[NumPy NN] Epoch 20/3500 - loss: 1.5643\n",
      "[NumPy NN] Epoch 30/3500 - loss: 1.5408\n",
      "[NumPy NN] Epoch 40/3500 - loss: 1.5221\n",
      "[NumPy NN] Epoch 50/3500 - loss: 1.5062\n",
      "[NumPy NN] Epoch 60/3500 - loss: 1.4915\n",
      "[NumPy NN] Epoch 70/3500 - loss: 1.4773\n",
      "[NumPy NN] Epoch 80/3500 - loss: 1.4632\n",
      "[NumPy NN] Epoch 90/3500 - loss: 1.4491\n",
      "[NumPy NN] Epoch 100/3500 - loss: 1.4351\n",
      "[NumPy NN] Epoch 110/3500 - loss: 1.4209\n",
      "[NumPy NN] Epoch 120/3500 - loss: 1.4067\n",
      "[NumPy NN] Epoch 130/3500 - loss: 1.3923\n",
      "[NumPy NN] Epoch 140/3500 - loss: 1.3778\n",
      "[NumPy NN] Epoch 150/3500 - loss: 1.3632\n",
      "[NumPy NN] Epoch 160/3500 - loss: 1.3484\n",
      "[NumPy NN] Epoch 170/3500 - loss: 1.3336\n",
      "[NumPy NN] Epoch 180/3500 - loss: 1.3189\n",
      "[NumPy NN] Epoch 190/3500 - loss: 1.3042\n",
      "[NumPy NN] Epoch 200/3500 - loss: 1.2898\n",
      "[NumPy NN] Epoch 210/3500 - loss: 1.2752\n",
      "[NumPy NN] Epoch 220/3500 - loss: 1.2606\n",
      "[NumPy NN] Epoch 230/3500 - loss: 1.2461\n",
      "[NumPy NN] Epoch 240/3500 - loss: 1.2316\n",
      "[NumPy NN] Epoch 250/3500 - loss: 1.2175\n",
      "[NumPy NN] Epoch 260/3500 - loss: 1.2036\n",
      "[NumPy NN] Epoch 270/3500 - loss: 1.1898\n",
      "[NumPy NN] Epoch 280/3500 - loss: 1.1765\n",
      "[NumPy NN] Epoch 290/3500 - loss: 1.1632\n",
      "[NumPy NN] Epoch 300/3500 - loss: 1.1503\n",
      "[NumPy NN] Epoch 310/3500 - loss: 1.1372\n",
      "[NumPy NN] Epoch 320/3500 - loss: 1.1245\n",
      "[NumPy NN] Epoch 330/3500 - loss: 1.1119\n",
      "[NumPy NN] Epoch 340/3500 - loss: 1.0994\n",
      "[NumPy NN] Epoch 350/3500 - loss: 1.0872\n",
      "[NumPy NN] Epoch 360/3500 - loss: 1.0751\n",
      "[NumPy NN] Epoch 370/3500 - loss: 1.0629\n",
      "[NumPy NN] Epoch 380/3500 - loss: 1.0510\n",
      "[NumPy NN] Epoch 390/3500 - loss: 1.0390\n",
      "[NumPy NN] Epoch 400/3500 - loss: 1.0274\n",
      "[NumPy NN] Epoch 410/3500 - loss: 1.0155\n",
      "[NumPy NN] Epoch 420/3500 - loss: 1.0045\n",
      "[NumPy NN] Epoch 430/3500 - loss: 0.9926\n",
      "[NumPy NN] Epoch 440/3500 - loss: 0.9812\n",
      "[NumPy NN] Epoch 450/3500 - loss: 0.9699\n",
      "[NumPy NN] Epoch 460/3500 - loss: 0.9584\n",
      "[NumPy NN] Epoch 470/3500 - loss: 0.9471\n",
      "[NumPy NN] Epoch 480/3500 - loss: 0.9357\n",
      "[NumPy NN] Epoch 490/3500 - loss: 0.9251\n",
      "[NumPy NN] Epoch 500/3500 - loss: 0.9147\n",
      "[NumPy NN] Epoch 510/3500 - loss: 0.9034\n",
      "[NumPy NN] Epoch 520/3500 - loss: 0.8932\n",
      "[NumPy NN] Epoch 530/3500 - loss: 0.8830\n",
      "[NumPy NN] Epoch 540/3500 - loss: 0.8727\n",
      "[NumPy NN] Epoch 550/3500 - loss: 0.8628\n",
      "[NumPy NN] Epoch 560/3500 - loss: 0.8523\n",
      "[NumPy NN] Epoch 570/3500 - loss: 0.8424\n",
      "[NumPy NN] Epoch 580/3500 - loss: 0.8333\n",
      "[NumPy NN] Epoch 590/3500 - loss: 0.8235\n",
      "[NumPy NN] Epoch 600/3500 - loss: 0.8132\n",
      "[NumPy NN] Epoch 610/3500 - loss: 0.8035\n",
      "[NumPy NN] Epoch 620/3500 - loss: 0.7944\n",
      "[NumPy NN] Epoch 630/3500 - loss: 0.7846\n",
      "[NumPy NN] Epoch 640/3500 - loss: 0.7750\n",
      "[NumPy NN] Epoch 650/3500 - loss: 0.7659\n",
      "[NumPy NN] Epoch 660/3500 - loss: 0.7566\n",
      "[NumPy NN] Epoch 670/3500 - loss: 0.7480\n",
      "[NumPy NN] Epoch 680/3500 - loss: 0.7386\n",
      "[NumPy NN] Epoch 690/3500 - loss: 0.7298\n",
      "[NumPy NN] Epoch 700/3500 - loss: 0.7214\n",
      "[NumPy NN] Epoch 710/3500 - loss: 0.7126\n",
      "[NumPy NN] Epoch 720/3500 - loss: 0.7034\n",
      "[NumPy NN] Epoch 730/3500 - loss: 0.6950\n",
      "[NumPy NN] Epoch 740/3500 - loss: 0.6862\n",
      "[NumPy NN] Epoch 750/3500 - loss: 0.6782\n",
      "[NumPy NN] Epoch 760/3500 - loss: 0.6700\n",
      "[NumPy NN] Epoch 770/3500 - loss: 0.6619\n",
      "[NumPy NN] Epoch 780/3500 - loss: 0.6542\n",
      "[NumPy NN] Epoch 790/3500 - loss: 0.6458\n",
      "[NumPy NN] Epoch 800/3500 - loss: 0.6379\n",
      "[NumPy NN] Epoch 810/3500 - loss: 0.6304\n",
      "[NumPy NN] Epoch 820/3500 - loss: 0.6231\n",
      "[NumPy NN] Epoch 830/3500 - loss: 0.6152\n",
      "[NumPy NN] Epoch 840/3500 - loss: 0.6078\n",
      "[NumPy NN] Epoch 850/3500 - loss: 0.6013\n",
      "[NumPy NN] Epoch 860/3500 - loss: 0.5936\n",
      "[NumPy NN] Epoch 870/3500 - loss: 0.5862\n",
      "[NumPy NN] Epoch 880/3500 - loss: 0.5801\n",
      "[NumPy NN] Epoch 890/3500 - loss: 0.5731\n",
      "[NumPy NN] Epoch 900/3500 - loss: 0.5671\n",
      "[NumPy NN] Epoch 910/3500 - loss: 0.5602\n",
      "[NumPy NN] Epoch 920/3500 - loss: 0.5540\n",
      "[NumPy NN] Epoch 930/3500 - loss: 0.5478\n",
      "[NumPy NN] Epoch 940/3500 - loss: 0.5416\n",
      "[NumPy NN] Epoch 950/3500 - loss: 0.5357\n",
      "[NumPy NN] Epoch 960/3500 - loss: 0.5291\n",
      "[NumPy NN] Epoch 970/3500 - loss: 0.5241\n",
      "[NumPy NN] Epoch 980/3500 - loss: 0.5174\n",
      "[NumPy NN] Epoch 990/3500 - loss: 0.5122\n",
      "[NumPy NN] Epoch 1000/3500 - loss: 0.5072\n",
      "[NumPy NN] Epoch 1010/3500 - loss: 0.5016\n",
      "[NumPy NN] Epoch 1020/3500 - loss: 0.4954\n",
      "[NumPy NN] Epoch 1030/3500 - loss: 0.4909\n",
      "[NumPy NN] Epoch 1040/3500 - loss: 0.4853\n",
      "[NumPy NN] Epoch 1050/3500 - loss: 0.4804\n",
      "[NumPy NN] Epoch 1060/3500 - loss: 0.4757\n",
      "[NumPy NN] Epoch 1070/3500 - loss: 0.4709\n",
      "[NumPy NN] Epoch 1080/3500 - loss: 0.4660\n",
      "[NumPy NN] Epoch 1090/3500 - loss: 0.4610\n",
      "[NumPy NN] Epoch 1100/3500 - loss: 0.4563\n",
      "[NumPy NN] Epoch 1110/3500 - loss: 0.4513\n",
      "[NumPy NN] Epoch 1120/3500 - loss: 0.4463\n",
      "[NumPy NN] Epoch 1130/3500 - loss: 0.4427\n",
      "[NumPy NN] Epoch 1140/3500 - loss: 0.4380\n",
      "[NumPy NN] Epoch 1150/3500 - loss: 0.4334\n",
      "[NumPy NN] Epoch 1160/3500 - loss: 0.4287\n",
      "[NumPy NN] Epoch 1170/3500 - loss: 0.4249\n",
      "[NumPy NN] Epoch 1180/3500 - loss: 0.4212\n",
      "[NumPy NN] Epoch 1190/3500 - loss: 0.4166\n",
      "[NumPy NN] Epoch 1200/3500 - loss: 0.4122\n",
      "[NumPy NN] Epoch 1210/3500 - loss: 0.4087\n",
      "[NumPy NN] Epoch 1220/3500 - loss: 0.4048\n",
      "[NumPy NN] Epoch 1230/3500 - loss: 0.4016\n",
      "[NumPy NN] Epoch 1240/3500 - loss: 0.3972\n",
      "[NumPy NN] Epoch 1250/3500 - loss: 0.3932\n",
      "[NumPy NN] Epoch 1260/3500 - loss: 0.3910\n",
      "[NumPy NN] Epoch 1270/3500 - loss: 0.3871\n",
      "[NumPy NN] Epoch 1280/3500 - loss: 0.3837\n",
      "[NumPy NN] Epoch 1290/3500 - loss: 0.3794\n",
      "[NumPy NN] Epoch 1300/3500 - loss: 0.3767\n",
      "[NumPy NN] Epoch 1310/3500 - loss: 0.3730\n",
      "[NumPy NN] Epoch 1320/3500 - loss: 0.3706\n",
      "[NumPy NN] Epoch 1330/3500 - loss: 0.3664\n",
      "[NumPy NN] Epoch 1340/3500 - loss: 0.3639\n",
      "[NumPy NN] Epoch 1350/3500 - loss: 0.3609\n",
      "[NumPy NN] Epoch 1360/3500 - loss: 0.3578\n",
      "[NumPy NN] Epoch 1370/3500 - loss: 0.3534\n",
      "[NumPy NN] Epoch 1380/3500 - loss: 0.3513\n",
      "[NumPy NN] Epoch 1390/3500 - loss: 0.3473\n",
      "[NumPy NN] Epoch 1400/3500 - loss: 0.3446\n",
      "[NumPy NN] Epoch 1410/3500 - loss: 0.3429\n",
      "[NumPy NN] Epoch 1420/3500 - loss: 0.3404\n",
      "[NumPy NN] Epoch 1430/3500 - loss: 0.3370\n",
      "[NumPy NN] Epoch 1440/3500 - loss: 0.3346\n",
      "[NumPy NN] Epoch 1450/3500 - loss: 0.3320\n",
      "[NumPy NN] Epoch 1460/3500 - loss: 0.3279\n",
      "[NumPy NN] Epoch 1470/3500 - loss: 0.3261\n",
      "[NumPy NN] Epoch 1480/3500 - loss: 0.3231\n",
      "[NumPy NN] Epoch 1490/3500 - loss: 0.3201\n",
      "[NumPy NN] Epoch 1500/3500 - loss: 0.3169\n",
      "[NumPy NN] Epoch 1510/3500 - loss: 0.3144\n",
      "[NumPy NN] Epoch 1520/3500 - loss: 0.3136\n",
      "[NumPy NN] Epoch 1530/3500 - loss: 0.3094\n",
      "[NumPy NN] Epoch 1540/3500 - loss: 0.3061\n",
      "[NumPy NN] Epoch 1550/3500 - loss: 0.3046\n",
      "[NumPy NN] Epoch 1560/3500 - loss: 0.3030\n",
      "[NumPy NN] Epoch 1570/3500 - loss: 0.3017\n",
      "[NumPy NN] Epoch 1580/3500 - loss: 0.2971\n",
      "[NumPy NN] Epoch 1590/3500 - loss: 0.2958\n",
      "[NumPy NN] Epoch 1600/3500 - loss: 0.2933\n",
      "[NumPy NN] Epoch 1610/3500 - loss: 0.2904\n",
      "[NumPy NN] Epoch 1620/3500 - loss: 0.2878\n",
      "[NumPy NN] Epoch 1630/3500 - loss: 0.2861\n",
      "[NumPy NN] Epoch 1640/3500 - loss: 0.2843\n",
      "[NumPy NN] Epoch 1650/3500 - loss: 0.2812\n",
      "[NumPy NN] Epoch 1660/3500 - loss: 0.2794\n",
      "[NumPy NN] Epoch 1670/3500 - loss: 0.2771\n",
      "[NumPy NN] Epoch 1680/3500 - loss: 0.2764\n",
      "[NumPy NN] Epoch 1690/3500 - loss: 0.2747\n",
      "[NumPy NN] Epoch 1700/3500 - loss: 0.2708\n",
      "[NumPy NN] Epoch 1710/3500 - loss: 0.2689\n",
      "[NumPy NN] Epoch 1720/3500 - loss: 0.2683\n",
      "[NumPy NN] Epoch 1730/3500 - loss: 0.2649\n",
      "[NumPy NN] Epoch 1740/3500 - loss: 0.2629\n",
      "[NumPy NN] Epoch 1750/3500 - loss: 0.2616\n",
      "[NumPy NN] Epoch 1760/3500 - loss: 0.2594\n",
      "[NumPy NN] Epoch 1770/3500 - loss: 0.2565\n",
      "[NumPy NN] Epoch 1780/3500 - loss: 0.2548\n",
      "[NumPy NN] Epoch 1790/3500 - loss: 0.2523\n",
      "[NumPy NN] Epoch 1800/3500 - loss: 0.2529\n",
      "[NumPy NN] Epoch 1810/3500 - loss: 0.2495\n",
      "[NumPy NN] Epoch 1820/3500 - loss: 0.2496\n",
      "[NumPy NN] Epoch 1830/3500 - loss: 0.2461\n",
      "[NumPy NN] Epoch 1840/3500 - loss: 0.2439\n",
      "[NumPy NN] Epoch 1850/3500 - loss: 0.2430\n",
      "[NumPy NN] Epoch 1860/3500 - loss: 0.2409\n",
      "[NumPy NN] Epoch 1870/3500 - loss: 0.2387\n",
      "[NumPy NN] Epoch 1880/3500 - loss: 0.2365\n",
      "[NumPy NN] Epoch 1890/3500 - loss: 0.2373\n",
      "[NumPy NN] Epoch 1900/3500 - loss: 0.2342\n",
      "[NumPy NN] Epoch 1910/3500 - loss: 0.2336\n",
      "[NumPy NN] Epoch 1920/3500 - loss: 0.2305\n",
      "[NumPy NN] Epoch 1930/3500 - loss: 0.2292\n",
      "[NumPy NN] Epoch 1940/3500 - loss: 0.2273\n",
      "[NumPy NN] Epoch 1950/3500 - loss: 0.2246\n",
      "[NumPy NN] Epoch 1960/3500 - loss: 0.2252\n",
      "[NumPy NN] Epoch 1970/3500 - loss: 0.2223\n",
      "[NumPy NN] Epoch 1980/3500 - loss: 0.2213\n",
      "[NumPy NN] Epoch 1990/3500 - loss: 0.2202\n",
      "[NumPy NN] Epoch 2000/3500 - loss: 0.2177\n",
      "[NumPy NN] Epoch 2010/3500 - loss: 0.2165\n",
      "[NumPy NN] Epoch 2020/3500 - loss: 0.2161\n",
      "[NumPy NN] Epoch 2030/3500 - loss: 0.2136\n",
      "[NumPy NN] Epoch 2040/3500 - loss: 0.2115\n",
      "[NumPy NN] Epoch 2050/3500 - loss: 0.2104\n",
      "[NumPy NN] Epoch 2060/3500 - loss: 0.2100\n",
      "[NumPy NN] Epoch 2070/3500 - loss: 0.2094\n",
      "[NumPy NN] Epoch 2080/3500 - loss: 0.2066\n",
      "[NumPy NN] Epoch 2090/3500 - loss: 0.2067\n",
      "[NumPy NN] Epoch 2100/3500 - loss: 0.2043\n",
      "[NumPy NN] Epoch 2110/3500 - loss: 0.2029\n",
      "[NumPy NN] Epoch 2120/3500 - loss: 0.2029\n",
      "[NumPy NN] Epoch 2130/3500 - loss: 0.2005\n",
      "[NumPy NN] Epoch 2140/3500 - loss: 0.1996\n",
      "[NumPy NN] Epoch 2150/3500 - loss: 0.1984\n",
      "[NumPy NN] Epoch 2160/3500 - loss: 0.1973\n",
      "[NumPy NN] Epoch 2170/3500 - loss: 0.1947\n",
      "[NumPy NN] Epoch 2180/3500 - loss: 0.1939\n",
      "[NumPy NN] Epoch 2190/3500 - loss: 0.1945\n",
      "[NumPy NN] Epoch 2200/3500 - loss: 0.1916\n",
      "[NumPy NN] Epoch 2210/3500 - loss: 0.1919\n",
      "[NumPy NN] Epoch 2220/3500 - loss: 0.1909\n",
      "[NumPy NN] Epoch 2230/3500 - loss: 0.1880\n",
      "[NumPy NN] Epoch 2240/3500 - loss: 0.1881\n",
      "[NumPy NN] Epoch 2250/3500 - loss: 0.1844\n",
      "[NumPy NN] Epoch 2260/3500 - loss: 0.1859\n",
      "[NumPy NN] Epoch 2270/3500 - loss: 0.1870\n",
      "[NumPy NN] Epoch 2280/3500 - loss: 0.1831\n",
      "[NumPy NN] Epoch 2290/3500 - loss: 0.1814\n",
      "[NumPy NN] Epoch 2300/3500 - loss: 0.1818\n",
      "[NumPy NN] Epoch 2310/3500 - loss: 0.1816\n",
      "[NumPy NN] Epoch 2320/3500 - loss: 0.1796\n",
      "[NumPy NN] Epoch 2330/3500 - loss: 0.1776\n",
      "[NumPy NN] Epoch 2340/3500 - loss: 0.1773\n",
      "[NumPy NN] Epoch 2350/3500 - loss: 0.1739\n",
      "[NumPy NN] Epoch 2360/3500 - loss: 0.1747\n",
      "[NumPy NN] Epoch 2370/3500 - loss: 0.1740\n",
      "[NumPy NN] Epoch 2380/3500 - loss: 0.1745\n",
      "[NumPy NN] Epoch 2390/3500 - loss: 0.1729\n",
      "[NumPy NN] Epoch 2400/3500 - loss: 0.1704\n",
      "[NumPy NN] Epoch 2410/3500 - loss: 0.1698\n",
      "[NumPy NN] Epoch 2420/3500 - loss: 0.1726\n",
      "[NumPy NN] Epoch 2430/3500 - loss: 0.1680\n",
      "[NumPy NN] Epoch 2440/3500 - loss: 0.1665\n",
      "[NumPy NN] Epoch 2450/3500 - loss: 0.1657\n",
      "[NumPy NN] Epoch 2460/3500 - loss: 0.1637\n",
      "[NumPy NN] Epoch 2470/3500 - loss: 0.1641\n",
      "[NumPy NN] Epoch 2480/3500 - loss: 0.1636\n",
      "[NumPy NN] Epoch 2490/3500 - loss: 0.1617\n",
      "[NumPy NN] Epoch 2500/3500 - loss: 0.1618\n",
      "[NumPy NN] Epoch 2510/3500 - loss: 0.1614\n",
      "[NumPy NN] Epoch 2520/3500 - loss: 0.1605\n",
      "[NumPy NN] Epoch 2530/3500 - loss: 0.1595\n",
      "[NumPy NN] Epoch 2540/3500 - loss: 0.1581\n",
      "[NumPy NN] Epoch 2550/3500 - loss: 0.1576\n",
      "[NumPy NN] Epoch 2560/3500 - loss: 0.1553\n",
      "[NumPy NN] Epoch 2570/3500 - loss: 0.1567\n",
      "[NumPy NN] Epoch 2580/3500 - loss: 0.1565\n",
      "[NumPy NN] Epoch 2590/3500 - loss: 0.1556\n",
      "[NumPy NN] Epoch 2600/3500 - loss: 0.1527\n",
      "[NumPy NN] Epoch 2610/3500 - loss: 0.1537\n",
      "[NumPy NN] Epoch 2620/3500 - loss: 0.1531\n",
      "[NumPy NN] Epoch 2630/3500 - loss: 0.1529\n",
      "[NumPy NN] Epoch 2640/3500 - loss: 0.1519\n",
      "[NumPy NN] Epoch 2650/3500 - loss: 0.1515\n",
      "[NumPy NN] Epoch 2660/3500 - loss: 0.1490\n",
      "[NumPy NN] Epoch 2670/3500 - loss: 0.1490\n",
      "[NumPy NN] Epoch 2680/3500 - loss: 0.1470\n",
      "[NumPy NN] Epoch 2690/3500 - loss: 0.1464\n",
      "[NumPy NN] Epoch 2700/3500 - loss: 0.1486\n",
      "[NumPy NN] Epoch 2710/3500 - loss: 0.1444\n",
      "[NumPy NN] Epoch 2720/3500 - loss: 0.1466\n",
      "[NumPy NN] Epoch 2730/3500 - loss: 0.1454\n",
      "[NumPy NN] Epoch 2740/3500 - loss: 0.1462\n",
      "[NumPy NN] Epoch 2750/3500 - loss: 0.1426\n",
      "[NumPy NN] Epoch 2760/3500 - loss: 0.1435\n",
      "[NumPy NN] Epoch 2770/3500 - loss: 0.1415\n",
      "[NumPy NN] Epoch 2780/3500 - loss: 0.1428\n",
      "[NumPy NN] Epoch 2790/3500 - loss: 0.1394\n",
      "[NumPy NN] Epoch 2800/3500 - loss: 0.1402\n",
      "[NumPy NN] Epoch 2810/3500 - loss: 0.1397\n",
      "[NumPy NN] Epoch 2820/3500 - loss: 0.1393\n",
      "[NumPy NN] Epoch 2830/3500 - loss: 0.1384\n",
      "[NumPy NN] Epoch 2840/3500 - loss: 0.1379\n",
      "[NumPy NN] Epoch 2850/3500 - loss: 0.1374\n",
      "[NumPy NN] Epoch 2860/3500 - loss: 0.1343\n",
      "[NumPy NN] Epoch 2870/3500 - loss: 0.1357\n",
      "[NumPy NN] Epoch 2880/3500 - loss: 0.1345\n",
      "[NumPy NN] Epoch 2890/3500 - loss: 0.1337\n",
      "[NumPy NN] Epoch 2900/3500 - loss: 0.1351\n",
      "[NumPy NN] Epoch 2910/3500 - loss: 0.1335\n",
      "[NumPy NN] Epoch 2920/3500 - loss: 0.1321\n",
      "[NumPy NN] Epoch 2930/3500 - loss: 0.1310\n",
      "[NumPy NN] Epoch 2940/3500 - loss: 0.1332\n",
      "[NumPy NN] Epoch 2950/3500 - loss: 0.1306\n",
      "[NumPy NN] Epoch 2960/3500 - loss: 0.1309\n",
      "[NumPy NN] Epoch 2970/3500 - loss: 0.1301\n",
      "[NumPy NN] Epoch 2980/3500 - loss: 0.1297\n",
      "[NumPy NN] Epoch 2990/3500 - loss: 0.1285\n",
      "[NumPy NN] Epoch 3000/3500 - loss: 0.1262\n",
      "[NumPy NN] Epoch 3010/3500 - loss: 0.1261\n",
      "[NumPy NN] Epoch 3020/3500 - loss: 0.1263\n",
      "[NumPy NN] Epoch 3030/3500 - loss: 0.1266\n",
      "[NumPy NN] Epoch 3040/3500 - loss: 0.1249\n",
      "[NumPy NN] Epoch 3050/3500 - loss: 0.1256\n",
      "[NumPy NN] Epoch 3060/3500 - loss: 0.1231\n",
      "[NumPy NN] Epoch 3070/3500 - loss: 0.1220\n",
      "[NumPy NN] Epoch 3080/3500 - loss: 0.1234\n",
      "[NumPy NN] Epoch 3090/3500 - loss: 0.1220\n",
      "[NumPy NN] Epoch 3100/3500 - loss: 0.1232\n",
      "[NumPy NN] Epoch 3110/3500 - loss: 0.1232\n",
      "[NumPy NN] Epoch 3120/3500 - loss: 0.1211\n",
      "[NumPy NN] Epoch 3130/3500 - loss: 0.1214\n",
      "[NumPy NN] Epoch 3140/3500 - loss: 0.1218\n",
      "[NumPy NN] Epoch 3150/3500 - loss: 0.1200\n",
      "[NumPy NN] Epoch 3160/3500 - loss: 0.1190\n",
      "[NumPy NN] Epoch 3170/3500 - loss: 0.1180\n",
      "[NumPy NN] Epoch 3180/3500 - loss: 0.1181\n",
      "[NumPy NN] Epoch 3190/3500 - loss: 0.1191\n",
      "[NumPy NN] Epoch 3200/3500 - loss: 0.1172\n",
      "[NumPy NN] Epoch 3210/3500 - loss: 0.1180\n",
      "[NumPy NN] Epoch 3220/3500 - loss: 0.1157\n",
      "[NumPy NN] Epoch 3230/3500 - loss: 0.1167\n",
      "[NumPy NN] Epoch 3240/3500 - loss: 0.1139\n",
      "[NumPy NN] Epoch 3250/3500 - loss: 0.1144\n",
      "[NumPy NN] Epoch 3260/3500 - loss: 0.1128\n",
      "[NumPy NN] Epoch 3270/3500 - loss: 0.1136\n",
      "[NumPy NN] Epoch 3280/3500 - loss: 0.1140\n",
      "[NumPy NN] Epoch 3290/3500 - loss: 0.1134\n",
      "[NumPy NN] Epoch 3300/3500 - loss: 0.1099\n",
      "[NumPy NN] Epoch 3310/3500 - loss: 0.1101\n",
      "[NumPy NN] Epoch 3320/3500 - loss: 0.1107\n",
      "[NumPy NN] Epoch 3330/3500 - loss: 0.1122\n",
      "[NumPy NN] Epoch 3340/3500 - loss: 0.1106\n",
      "[NumPy NN] Epoch 3350/3500 - loss: 0.1110\n",
      "[NumPy NN] Epoch 3360/3500 - loss: 0.1073\n",
      "[NumPy NN] Epoch 3370/3500 - loss: 0.1087\n",
      "[NumPy NN] Epoch 3380/3500 - loss: 0.1096\n",
      "[NumPy NN] Epoch 3390/3500 - loss: 0.1072\n",
      "[NumPy NN] Epoch 3400/3500 - loss: 0.1073\n",
      "[NumPy NN] Epoch 3410/3500 - loss: 0.1069\n",
      "[NumPy NN] Epoch 3420/3500 - loss: 0.1074\n",
      "[NumPy NN] Epoch 3430/3500 - loss: 0.1077\n",
      "[NumPy NN] Epoch 3440/3500 - loss: 0.1073\n",
      "[NumPy NN] Epoch 3450/3500 - loss: 0.1058\n",
      "[NumPy NN] Epoch 3460/3500 - loss: 0.1034\n",
      "[NumPy NN] Epoch 3470/3500 - loss: 0.1041\n",
      "[NumPy NN] Epoch 3480/3500 - loss: 0.1043\n",
      "[NumPy NN] Epoch 3490/3500 - loss: 0.1047\n",
      "[NumPy NN] Epoch 3500/3500 - loss: 0.1032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8065081819490179,\n",
       " 1.7192379708177592,\n",
       " 1.6790574449412903,\n",
       " 1.6552295072035323,\n",
       " 1.6392199943800176,\n",
       " 1.6275777131080607,\n",
       " 1.6185496475739445,\n",
       " 1.6112117285250829,\n",
       " 1.605013898136764,\n",
       " 1.5996188576978214,\n",
       " 1.5948045983521195,\n",
       " 1.5904708646372379,\n",
       " 1.586491512879967,\n",
       " 1.5827952604197633,\n",
       " 1.5793146634845305,\n",
       " 1.5760287899677492,\n",
       " 1.5729017384381807,\n",
       " 1.5699116482313296,\n",
       " 1.5670487382675684,\n",
       " 1.5643034543654026,\n",
       " 1.5616413732206602,\n",
       " 1.5590654635793852,\n",
       " 1.5565644509329506,\n",
       " 1.5541375189664286,\n",
       " 1.5517662207199303,\n",
       " 1.5494624885214754,\n",
       " 1.547244762353699,\n",
       " 1.5450827182838136,\n",
       " 1.5429533102504707,\n",
       " 1.5408479399315262,\n",
       " 1.5388266660326555,\n",
       " 1.5368277897134712,\n",
       " 1.5348735181133066,\n",
       " 1.5329482540688482,\n",
       " 1.531045149922974,\n",
       " 1.529213463253896,\n",
       " 1.527400844261906,\n",
       " 1.5256336357310414,\n",
       " 1.5238709004045632,\n",
       " 1.5221483382537988,\n",
       " 1.5204525134976272,\n",
       " 1.5187611044590599,\n",
       " 1.5171374918224958,\n",
       " 1.5154737591003493,\n",
       " 1.5139130035302895,\n",
       " 1.5123218636528575,\n",
       " 1.5107537931896582,\n",
       " 1.5092025530366164,\n",
       " 1.507647775817585,\n",
       " 1.5061511915694734,\n",
       " 1.5046452406330209,\n",
       " 1.503137262564128,\n",
       " 1.5016587413480686,\n",
       " 1.5001747227662199,\n",
       " 1.4987115451561024,\n",
       " 1.4972369786756123,\n",
       " 1.4957926873986553,\n",
       " 1.4943369396893225,\n",
       " 1.492897396413026,\n",
       " 1.4914633742229861,\n",
       " 1.4900366790469457,\n",
       " 1.4886246772656466,\n",
       " 1.4872091768784073,\n",
       " 1.4857973022999975,\n",
       " 1.4843569244526418,\n",
       " 1.4829501005660644,\n",
       " 1.4815753503840574,\n",
       " 1.4801316426388322,\n",
       " 1.4787404805642441,\n",
       " 1.477312484663364,\n",
       " 1.475920673594312,\n",
       " 1.47451046455201,\n",
       " 1.473093515844364,\n",
       " 1.47167180072943,\n",
       " 1.4702674015021444,\n",
       " 1.4688538719024713,\n",
       " 1.4674559260313402,\n",
       " 1.4660437378074853,\n",
       " 1.4646000657026592,\n",
       " 1.463233539603199,\n",
       " 1.4618356970698856,\n",
       " 1.460430182786165,\n",
       " 1.4590024840187488,\n",
       " 1.4576178193305704,\n",
       " 1.4562100897378583,\n",
       " 1.4548124894888652,\n",
       " 1.453393942857518,\n",
       " 1.451978075386304,\n",
       " 1.4505775512248589,\n",
       " 1.449147050840952,\n",
       " 1.4477949760473976,\n",
       " 1.4463631498748064,\n",
       " 1.4449779707909138,\n",
       " 1.443565067003055,\n",
       " 1.4422038439476847,\n",
       " 1.44076000204551,\n",
       " 1.4393752827784407,\n",
       " 1.437942847764697,\n",
       " 1.4365643189167472,\n",
       " 1.435086952708995,\n",
       " 1.4337293934265256,\n",
       " 1.4322694775001392,\n",
       " 1.4308619137661096,\n",
       " 1.4294508488240514,\n",
       " 1.428066397244574,\n",
       " 1.4266473398248816,\n",
       " 1.425220330250436,\n",
       " 1.4237793445629996,\n",
       " 1.4223685260985997,\n",
       " 1.4209147921894938,\n",
       " 1.4195252849778377,\n",
       " 1.4180835700261019,\n",
       " 1.4166516146043302,\n",
       " 1.4152727191283403,\n",
       " 1.413821152821879,\n",
       " 1.4124047390572774,\n",
       " 1.4110020595246155,\n",
       " 1.4095611434184903,\n",
       " 1.4081488239662516,\n",
       " 1.4066752057849323,\n",
       " 1.4052903278656574,\n",
       " 1.403844545722108,\n",
       " 1.4024098020626565,\n",
       " 1.4009715833889271,\n",
       " 1.39954590708182,\n",
       " 1.3981086278856036,\n",
       " 1.3966331547026678,\n",
       " 1.3952259670360843,\n",
       " 1.3937380273091522,\n",
       " 1.392335658937753,\n",
       " 1.390906894979153,\n",
       " 1.3893952133688652,\n",
       " 1.3880065440628027,\n",
       " 1.3865677047418286,\n",
       " 1.3850774746409418,\n",
       " 1.3836498729943554,\n",
       " 1.3821652971826255,\n",
       " 1.3807026334823895,\n",
       " 1.37926435579364,\n",
       " 1.3778248962169217,\n",
       " 1.376348398061494,\n",
       " 1.3748980823524277,\n",
       " 1.3733657413710725,\n",
       " 1.3719520022251979,\n",
       " 1.370433670281985,\n",
       " 1.3690258157866066,\n",
       " 1.3675834164988114,\n",
       " 1.3661069329398856,\n",
       " 1.3646030015812793,\n",
       " 1.3631960308505529,\n",
       " 1.361699062787056,\n",
       " 1.360239809188697,\n",
       " 1.3587495590492746,\n",
       " 1.357247520152149,\n",
       " 1.355767793133363,\n",
       " 1.3543093372270896,\n",
       " 1.3527588650291673,\n",
       " 1.351360557103169,\n",
       " 1.3498782832944045,\n",
       " 1.348385213378893,\n",
       " 1.346924799694618,\n",
       " 1.3453810551673908,\n",
       " 1.343866687230957,\n",
       " 1.3424832643182223,\n",
       " 1.3409967667213016,\n",
       " 1.339519206385941,\n",
       " 1.337997299749317,\n",
       " 1.3365549879380627,\n",
       " 1.3350929855361882,\n",
       " 1.333597204523443,\n",
       " 1.3321459857055604,\n",
       " 1.3306508507088382,\n",
       " 1.3291632562241589,\n",
       " 1.3277278155308274,\n",
       " 1.3261932714972762,\n",
       " 1.3247026446469705,\n",
       " 1.323260580797962,\n",
       " 1.321777120683045,\n",
       " 1.3203191532141136,\n",
       " 1.3188841291995472,\n",
       " 1.3173464453668395,\n",
       " 1.31597197454755,\n",
       " 1.314509379899407,\n",
       " 1.313024953241512,\n",
       " 1.311595827397897,\n",
       " 1.3100932639416447,\n",
       " 1.3086837493721255,\n",
       " 1.3072138246792409,\n",
       " 1.3057074373552724,\n",
       " 1.3042232801450477,\n",
       " 1.3028856851056676,\n",
       " 1.3013903044109145,\n",
       " 1.2998505726372356,\n",
       " 1.2984791684623254,\n",
       " 1.2970316611963175,\n",
       " 1.295500131824426,\n",
       " 1.294103027806723,\n",
       " 1.2926778003259203,\n",
       " 1.2911963076515958,\n",
       " 1.2897664181540596,\n",
       " 1.2883003346455657,\n",
       " 1.2868205278269276,\n",
       " 1.2853931901789137,\n",
       " 1.283910576432808,\n",
       " 1.282438765548413,\n",
       " 1.2809100348782791,\n",
       " 1.2795011488943078,\n",
       " 1.2780602440974016,\n",
       " 1.2765556214868077,\n",
       " 1.275170147022789,\n",
       " 1.273649811191539,\n",
       " 1.2721712700796537,\n",
       " 1.2707462251162536,\n",
       " 1.2692825557584504,\n",
       " 1.267720538693391,\n",
       " 1.2664015318348865,\n",
       " 1.264911701211109,\n",
       " 1.2634967825294647,\n",
       " 1.262012047150974,\n",
       " 1.26056420519569,\n",
       " 1.2590579764893775,\n",
       " 1.2576371911781654,\n",
       " 1.2562212331602647,\n",
       " 1.2547140067317755,\n",
       " 1.2532228695612588,\n",
       " 1.2517273965461952,\n",
       " 1.250356613097251,\n",
       " 1.2489188568875709,\n",
       " 1.2474600916323093,\n",
       " 1.2461006620792499,\n",
       " 1.2445417341354723,\n",
       " 1.2430713679376182,\n",
       " 1.2417162759817937,\n",
       " 1.2402440735315385,\n",
       " 1.2387803750541873,\n",
       " 1.2373612851472735,\n",
       " 1.2359727212852971,\n",
       " 1.2344196722450138,\n",
       " 1.2331534236403834,\n",
       " 1.2315925473118174,\n",
       " 1.2300940664254034,\n",
       " 1.2287709570954188,\n",
       " 1.2274716814982776,\n",
       " 1.2259454917127552,\n",
       " 1.2245797989551366,\n",
       " 1.2230959021231207,\n",
       " 1.2218339269409688,\n",
       " 1.2203004944378968,\n",
       " 1.2189140320002025,\n",
       " 1.2175384688952182,\n",
       " 1.2160739010028994,\n",
       " 1.2146768767511777,\n",
       " 1.2134305737444695,\n",
       " 1.211910460368818,\n",
       " 1.2105490818112663,\n",
       " 1.2091499309962572,\n",
       " 1.207793952285367,\n",
       " 1.2062814571163916,\n",
       " 1.2049669326106376,\n",
       " 1.2036058402638898,\n",
       " 1.2021855918770696,\n",
       " 1.200788603924596,\n",
       " 1.1994084525554147,\n",
       " 1.1980826332857781,\n",
       " 1.19664691902218,\n",
       " 1.1952989295704788,\n",
       " 1.1939727780377067,\n",
       " 1.1924282303590172,\n",
       " 1.1912267401077994,\n",
       " 1.18979386193704,\n",
       " 1.1885369741535412,\n",
       " 1.1871574265245517,\n",
       " 1.1858223997560744,\n",
       " 1.1843960722381488,\n",
       " 1.1831618357397469,\n",
       " 1.1816525235140476,\n",
       " 1.180461048415556,\n",
       " 1.179029752742166,\n",
       " 1.1777963960886288,\n",
       " 1.176489791530404,\n",
       " 1.1748428377946407,\n",
       " 1.1737667220720094,\n",
       " 1.1723856741731538,\n",
       " 1.1712526612573104,\n",
       " 1.1698376538481394,\n",
       " 1.1685055896777792,\n",
       " 1.1671848687103568,\n",
       " 1.1659520702981352,\n",
       " 1.1645918502075308,\n",
       " 1.163210559661076,\n",
       " 1.1619074497249164,\n",
       " 1.1606543397202516,\n",
       " 1.1592105587449901,\n",
       " 1.157916506664261,\n",
       " 1.1567321977396698,\n",
       " 1.1553686208493763,\n",
       " 1.153995125940808,\n",
       " 1.1526803952558662,\n",
       " 1.1512688544856136,\n",
       " 1.1502546600670922,\n",
       " 1.1489201388955874,\n",
       " 1.1475643140422471,\n",
       " 1.1462387099223936,\n",
       " 1.1449642279349699,\n",
       " 1.14350900923878,\n",
       " 1.1425800014467193,\n",
       " 1.1411599112749147,\n",
       " 1.139644591835226,\n",
       " 1.1385490496912125,\n",
       " 1.1371732431605575,\n",
       " 1.1357765751062572,\n",
       " 1.1345532213629936,\n",
       " 1.133418515570534,\n",
       " 1.132031640642311,\n",
       " 1.1308026731157597,\n",
       " 1.1295530923831403,\n",
       " 1.1281867627530697,\n",
       " 1.1269936303531152,\n",
       " 1.1257077063054959,\n",
       " 1.1244926856802784,\n",
       " 1.123273870132257,\n",
       " 1.1219842479737816,\n",
       " 1.1206018183404793,\n",
       " 1.1194334200619813,\n",
       " 1.1182135252738767,\n",
       " 1.1169564760072435,\n",
       " 1.1156972824876796,\n",
       " 1.1143708117739395,\n",
       " 1.1132798370811212,\n",
       " 1.1118925037535037,\n",
       " 1.110584233223994,\n",
       " 1.1095020613937223,\n",
       " 1.1081993836889923,\n",
       " 1.1068862559047268,\n",
       " 1.105710769009467,\n",
       " 1.104456234853895,\n",
       " 1.103167051143011,\n",
       " 1.1020225002317412,\n",
       " 1.1007631206726842,\n",
       " 1.0994221887231712,\n",
       " 1.0982744626782501,\n",
       " 1.096890394356712,\n",
       " 1.095792851315945,\n",
       " 1.0944027511404666,\n",
       " 1.093303274347023,\n",
       " 1.092279640322917,\n",
       " 1.0909361118139358,\n",
       " 1.0896208740117068,\n",
       " 1.0882141962775362,\n",
       " 1.0872227852178475,\n",
       " 1.0858968625596264,\n",
       " 1.0846787361107408,\n",
       " 1.0833492053502318,\n",
       " 1.0824306979059592,\n",
       " 1.0811420826678941,\n",
       " 1.0800336034138711,\n",
       " 1.078755091485967,\n",
       " 1.0774218273343343,\n",
       " 1.0763248454924896,\n",
       " 1.075066532814006,\n",
       " 1.0738123449763417,\n",
       " 1.0726743277326225,\n",
       " 1.0715371825462476,\n",
       " 1.070239153979029,\n",
       " 1.0691364923104512,\n",
       " 1.067932657293915,\n",
       " 1.0666992323040967,\n",
       " 1.0655526648354268,\n",
       " 1.0643424312188519,\n",
       " 1.0628707969496722,\n",
       " 1.061891028718571,\n",
       " 1.0606599860113792,\n",
       " 1.0594585521069282,\n",
       " 1.0582516477016808,\n",
       " 1.0570110728899458,\n",
       " 1.0559057321179148,\n",
       " 1.0547685933959423,\n",
       " 1.053412337531862,\n",
       " 1.0523052908566086,\n",
       " 1.0509774276828527,\n",
       " 1.0497816831133966,\n",
       " 1.0486413854506496,\n",
       " 1.0473618554404907,\n",
       " 1.0463941280017892,\n",
       " 1.0452524069516196,\n",
       " 1.0438918332257459,\n",
       " 1.042508232165635,\n",
       " 1.0413775301423445,\n",
       " 1.0403698994503254,\n",
       " 1.038987125694234,\n",
       " 1.0379641307364786,\n",
       " 1.0367306427368237,\n",
       " 1.0358109160238926,\n",
       " 1.034492828314073,\n",
       " 1.0331298320605908,\n",
       " 1.0322029183797845,\n",
       " 1.0307855962108616,\n",
       " 1.0298281045649025,\n",
       " 1.0285007924577534,\n",
       " 1.0273677497650775,\n",
       " 1.0262798135039706,\n",
       " 1.0249121283305587,\n",
       " 1.0237079549184815,\n",
       " 1.0228898954422205,\n",
       " 1.021573641775406,\n",
       " 1.0203363086361688,\n",
       " 1.019256785851536,\n",
       " 1.0180669277185639,\n",
       " 1.0168423026069757,\n",
       " 1.0155443348906095,\n",
       " 1.0145482279431508,\n",
       " 1.0135375915527938,\n",
       " 1.012296812531161,\n",
       " 1.0113396352803923,\n",
       " 1.0097714696947673,\n",
       " 1.0089052734906518,\n",
       " 1.0077067560836281,\n",
       " 1.0063859795325454,\n",
       " 1.0054580705060256,\n",
       " 1.0045390752741794,\n",
       " 1.0032013488032256,\n",
       " 1.001692454757961,\n",
       " 1.0008965302783244,\n",
       " 0.9999131692317718,\n",
       " 0.9985432644351189,\n",
       " 0.9976694655442192,\n",
       " 0.9963022670329869,\n",
       " 0.9949759604886043,\n",
       " 0.9941974006512108,\n",
       " 0.9925885638824888,\n",
       " 0.9915950330879552,\n",
       " 0.9905691151680529,\n",
       " 0.9891616734983387,\n",
       " 0.9883987809680398,\n",
       " 0.9871205639497292,\n",
       " 0.9860574647884485,\n",
       " 0.9848096229721133,\n",
       " 0.9836295221913068,\n",
       " 0.9822991296530887,\n",
       " 0.9811661403260639,\n",
       " 0.9803808212172213,\n",
       " 0.9788219156091523,\n",
       " 0.9780212728379714,\n",
       " 0.9767535195515755,\n",
       " 0.9752737005643392,\n",
       " 0.9746328843803357,\n",
       " 0.973237450094653,\n",
       " 0.9720446036441192,\n",
       " 0.9711102810085553,\n",
       " 0.9699162152964904,\n",
       " 0.96864553950989,\n",
       " 0.967579127022202,\n",
       " 0.9664933221865646,\n",
       " 0.9653866792809253,\n",
       " 0.96412703441586,\n",
       " 0.9630562320716288,\n",
       " 0.9617687284550615,\n",
       " 0.9608531370735232,\n",
       " 0.9593183983187583,\n",
       " 0.9584396661343157,\n",
       " 0.9573247823118093,\n",
       " 0.9561321217126547,\n",
       " 0.9549478234059524,\n",
       " 0.9539286674095604,\n",
       " 0.9527352084537294,\n",
       " 0.9516362466258218,\n",
       " 0.950181617915196,\n",
       " 0.9493003092160329,\n",
       " 0.948382147223941,\n",
       " 0.9470523407976533,\n",
       " 0.9459516962412947,\n",
       " 0.9448318379235006,\n",
       " 0.9439019979895832,\n",
       " 0.9424950898761061,\n",
       " 0.9418697097150209,\n",
       " 0.9404014111688307,\n",
       " 0.9393158317069146,\n",
       " 0.9380555987881186,\n",
       " 0.9372849760691196,\n",
       " 0.9357118775845679,\n",
       " 0.9350792295678428,\n",
       " 0.9336473904541691,\n",
       " 0.9327557503661101,\n",
       " 0.931510332391471,\n",
       " 0.9304600456530813,\n",
       " 0.9292030260341702,\n",
       " 0.9286078197278677,\n",
       " 0.9271669137350893,\n",
       " 0.9263082466458233,\n",
       " 0.9251145211924331,\n",
       " 0.9238306994368962,\n",
       " 0.9229962341472124,\n",
       " 0.9220108209109472,\n",
       " 0.9207328171050015,\n",
       " 0.9197208725290745,\n",
       " 0.9188576158458518,\n",
       " 0.9175535736060503,\n",
       " 0.9166358044540799,\n",
       " 0.9153826340066917,\n",
       " 0.9146807697094436,\n",
       " 0.913278233981988,\n",
       " 0.9120958365370956,\n",
       " 0.910898524322606,\n",
       " 0.9098328430270237,\n",
       " 0.909154924416711,\n",
       " 0.9076943580108733,\n",
       " 0.9070041115167276,\n",
       " 0.9056351692573182,\n",
       " 0.9043622771957898,\n",
       " 0.9033998817338142,\n",
       " 0.9024866425714801,\n",
       " 0.9014945987597176,\n",
       " 0.9000251989587843,\n",
       " 0.8996741611305641,\n",
       " 0.8983690035692309,\n",
       " 0.8974099381741145,\n",
       " 0.8963070687222154,\n",
       " 0.8952509386502936,\n",
       " 0.8941433338952892,\n",
       " 0.8932394003953888,\n",
       " 0.8921395529089405,\n",
       " 0.8910478801041785,\n",
       " 0.8902905818175099,\n",
       " 0.8891418535051344,\n",
       " 0.887849959437433,\n",
       " 0.8870171315733594,\n",
       " 0.8862858862555153,\n",
       " 0.8851765873014615,\n",
       " 0.8839012416468615,\n",
       " 0.8829589451669266,\n",
       " 0.8817814875541894,\n",
       " 0.8809234533418587,\n",
       " 0.8801531609241896,\n",
       " 0.8786616084246437,\n",
       " 0.8781868442668943,\n",
       " 0.8767026532827487,\n",
       " 0.8757692814449413,\n",
       " 0.8749985605968731,\n",
       " 0.8734772544732028,\n",
       " 0.8726974553810555,\n",
       " 0.871545227000764,\n",
       " 0.8706588404737555,\n",
       " 0.8698076583698466,\n",
       " 0.8688391269785731,\n",
       " 0.8677883271564776,\n",
       " 0.8665641305456246,\n",
       " 0.8659904591552924,\n",
       " 0.8647517049865259,\n",
       " 0.8636981610329164,\n",
       " 0.8628493589309848,\n",
       " 0.8618905529099459,\n",
       " 0.8606883059945075,\n",
       " 0.8594313051556538,\n",
       " 0.859175723060751,\n",
       " 0.8575931739682814,\n",
       " 0.8570644833181481,\n",
       " 0.8557615889777598,\n",
       " 0.8547601584903286,\n",
       " 0.8538898050301674,\n",
       " 0.852308845788139,\n",
       " 0.8515142742843609,\n",
       " 0.8507266921465109,\n",
       " 0.8495196127078825,\n",
       " 0.8483885159385476,\n",
       " 0.8475517106676068,\n",
       " 0.8468964592104392,\n",
       " 0.8455718651014945,\n",
       " 0.8447438581135726,\n",
       " 0.843731740414923,\n",
       " 0.8424391632127483,\n",
       " 0.8417585654495062,\n",
       " 0.8408594105781261,\n",
       " 0.8397659859848728,\n",
       " 0.8389497002206735,\n",
       " 0.8376828590159526,\n",
       " 0.8370868733945483,\n",
       " 0.8359711478509203,\n",
       " 0.8350745927310661,\n",
       " 0.8340558056236246,\n",
       " 0.8333314566817277,\n",
       " 0.8317905012274118,\n",
       " 0.8312464915075531,\n",
       " 0.8300464864534176,\n",
       " 0.8290539448991826,\n",
       " 0.828256118955171,\n",
       " 0.8270991004146374,\n",
       " 0.826367585232014,\n",
       " 0.8251211653447209,\n",
       " 0.8240949005670172,\n",
       " 0.823480814713052,\n",
       " 0.8223667343789353,\n",
       " 0.8211380280481897,\n",
       " 0.8201301335504849,\n",
       " 0.8193945196996499,\n",
       " 0.8183963118958151,\n",
       " 0.8171909018591178,\n",
       " 0.8162514503136252,\n",
       " 0.8153591291430577,\n",
       " 0.8146176608276466,\n",
       " 0.8132145094053845,\n",
       " 0.8124678746766649,\n",
       " 0.8115982817055839,\n",
       " 0.8103835384426233,\n",
       " 0.809567187008748,\n",
       " 0.8086661810339486,\n",
       " 0.8079203221761336,\n",
       " 0.8064147860242029,\n",
       " 0.8055644884766762,\n",
       " 0.8048375815280806,\n",
       " 0.8034880829927108,\n",
       " 0.8029489723856544,\n",
       " 0.8018387823253663,\n",
       " 0.8010425689129806,\n",
       " 0.7999483345225713,\n",
       " 0.7993665925121681,\n",
       " 0.7979873410761074,\n",
       " 0.7970384507885007,\n",
       " 0.7959624749652795,\n",
       " 0.7950604871712901,\n",
       " 0.7943645111101018,\n",
       " 0.7933295860638446,\n",
       " 0.7916170403286459,\n",
       " 0.7912153020057956,\n",
       " 0.7901246825844953,\n",
       " 0.7892717799453073,\n",
       " 0.7885115056448926,\n",
       " 0.7873978660688631,\n",
       " 0.7862255135036578,\n",
       " 0.7853235278821941,\n",
       " 0.7846288873283882,\n",
       " 0.7836916163133574,\n",
       " 0.7827274234958119,\n",
       " 0.7815204079939855,\n",
       " 0.7809208558344118,\n",
       " 0.779904951406504,\n",
       " 0.7790207655252701,\n",
       " 0.7781302895762897,\n",
       " 0.7768525611653185,\n",
       " 0.7764990641521392,\n",
       " 0.7749940505055399,\n",
       " 0.7742623944376,\n",
       " 0.7728290278470189,\n",
       " 0.7721176429106014,\n",
       " 0.77132563400187,\n",
       " 0.770697885445076,\n",
       " 0.7697535594540584,\n",
       " 0.7690861617397748,\n",
       " 0.7677879283438732,\n",
       " 0.7668397046738771,\n",
       " 0.7659060448082519,\n",
       " 0.7653999854696687,\n",
       " 0.7637834478022435,\n",
       " 0.7632402713148629,\n",
       " 0.7621017842634136,\n",
       " 0.7611006729416593,\n",
       " 0.7603570575349188,\n",
       " 0.7589555380873823,\n",
       " 0.7586702669317694,\n",
       " 0.7575989541928682,\n",
       " 0.7566146130042577,\n",
       " 0.7557447210773794,\n",
       " 0.7546000708271158,\n",
       " 0.7538265141238751,\n",
       " 0.7530173436062164,\n",
       " 0.7525502487394393,\n",
       " 0.751163364006823,\n",
       " 0.7505677164592233,\n",
       " 0.7497758952533844,\n",
       " 0.7484953066995732,\n",
       " 0.7479552419151151,\n",
       " 0.74686571904555,\n",
       " 0.7451993630226125,\n",
       " 0.7450005332713515,\n",
       " 0.7442636483003773,\n",
       " 0.7430363090456553,\n",
       " 0.7423805997293195,\n",
       " 0.7414158514479497,\n",
       " 0.7402086282126107,\n",
       " 0.7395760205274887,\n",
       " 0.738639968248923,\n",
       " 0.7377026598995909,\n",
       " 0.7371297630933452,\n",
       " 0.7362716124281178,\n",
       " 0.7352443102661402,\n",
       " 0.7344045210055193,\n",
       " 0.7334338664189974,\n",
       " 0.7325132808859961,\n",
       " 0.7319522083044427,\n",
       " 0.7305239039036078,\n",
       " 0.7298436104908096,\n",
       " 0.7288970902208819,\n",
       " 0.7278757770998505,\n",
       " 0.727203991889522,\n",
       " 0.7264219320098014,\n",
       " 0.7252731936891484,\n",
       " 0.7241221695789133,\n",
       " 0.7241287154099565,\n",
       " 0.7229066620502472,\n",
       " 0.7217869795235278,\n",
       " 0.7213911902597504,\n",
       " 0.7199713474458821,\n",
       " 0.7195484729192321,\n",
       " 0.7182468961238817,\n",
       " 0.717816694466397,\n",
       " 0.71698295442968,\n",
       " 0.7159092456441492,\n",
       " 0.7149931309524118,\n",
       " 0.7137455051379742,\n",
       " 0.7134665489555443,\n",
       " 0.7126054119395883,\n",
       " 0.7116271646268005,\n",
       " 0.7108355881780318,\n",
       " 0.7096661328246163,\n",
       " 0.7084551589425502,\n",
       " 0.7076654640491467,\n",
       " 0.7074089576013998,\n",
       " 0.7060090623517545,\n",
       " 0.7052795347145829,\n",
       " 0.7040859276683404,\n",
       " 0.7034285030147525,\n",
       " 0.7028877365599793,\n",
       " 0.7016557612250777,\n",
       " 0.7005862756823553,\n",
       " 0.7002029874733825,\n",
       " 0.6989734031944511,\n",
       " 0.6984904742956013,\n",
       " 0.6975426110821566,\n",
       " 0.6963896762041216,\n",
       " 0.6958325082481721,\n",
       " 0.6949994106298841,\n",
       " 0.694235212137475,\n",
       " 0.6935955587819909,\n",
       " 0.6925630728892208,\n",
       " 0.691273642897405,\n",
       " 0.6904069824857054,\n",
       " 0.6901774435744472,\n",
       " 0.6886877609619609,\n",
       " 0.6883186105780411,\n",
       " 0.6872881221347815,\n",
       " 0.6861838861725478,\n",
       " 0.6853668957690427,\n",
       " 0.6848769929386146,\n",
       " 0.6838736500278099,\n",
       " 0.6832075495504856,\n",
       " 0.6823555164582586,\n",
       " 0.6807923495898681,\n",
       " 0.680606137935229,\n",
       " 0.6799645016066591,\n",
       " 0.6788351741283525,\n",
       " 0.6782121573630177,\n",
       " 0.6776619177162244,\n",
       " 0.6765347690487097,\n",
       " 0.6754653533524101,\n",
       " 0.6746554615940015,\n",
       " 0.6741915172683058,\n",
       " 0.6731244642545466,\n",
       " 0.6726246971854748,\n",
       " 0.6708854891314835,\n",
       " 0.6708255745300735,\n",
       " 0.6699973228706729,\n",
       " 0.6693130203907915,\n",
       " 0.6684607976230977,\n",
       " 0.6678812506345937,\n",
       " 0.6668247413519646,\n",
       " 0.6660723970729264,\n",
       " 0.6649632817440064,\n",
       " 0.6644611151094877,\n",
       " 0.6635580052616447,\n",
       " 0.6620090871772178,\n",
       " 0.6618743651940863,\n",
       " 0.6610346992327865,\n",
       " 0.6605729176475373,\n",
       " 0.6589942927105692,\n",
       " 0.6584170356562817,\n",
       " 0.6576415328080798,\n",
       " 0.6569627347742603,\n",
       " 0.6564673417565198,\n",
       " 0.6551553514645577,\n",
       " 0.6548089565394748,\n",
       " 0.6542132915213814,\n",
       " 0.6529590649575394,\n",
       " 0.6521846729240296,\n",
       " 0.6516931779707985,\n",
       " 0.6504422156006245,\n",
       " 0.6498355738327196,\n",
       " 0.6493919182473639,\n",
       " 0.6477942216099044,\n",
       " 0.6471998732175512,\n",
       " 0.6468457215136897,\n",
       " 0.6458017154623674,\n",
       " 0.6454964556925661,\n",
       " 0.6446502946610612,\n",
       " 0.6435968714861348,\n",
       " 0.6427962890640315,\n",
       " 0.6421171960723578,\n",
       " 0.640605617980699,\n",
       " 0.6406376460421421,\n",
       " 0.6394822880924927,\n",
       " 0.6388854622417424,\n",
       " 0.6378601397132023,\n",
       " 0.6370566674845562,\n",
       " 0.6365957663487778,\n",
       " 0.6358099924006131,\n",
       " 0.6346239659307137,\n",
       " 0.6347645444366814,\n",
       " 0.633042610089884,\n",
       " 0.6327517217659299,\n",
       " 0.6321162481378246,\n",
       " 0.6310773256550525,\n",
       " 0.6304310399581807,\n",
       " 0.6291647915751041,\n",
       " 0.6289279249422393,\n",
       " 0.6280435057842351,\n",
       " 0.6276152564012932,\n",
       " 0.6264264589285372,\n",
       " 0.6256452748636661,\n",
       " 0.6253782568682666,\n",
       " 0.6246881108789234,\n",
       " 0.6235557343935832,\n",
       " 0.6230868537937652,\n",
       " 0.6220668278491535,\n",
       " 0.6214565001569208,\n",
       " 0.6204381405020707,\n",
       " 0.6202287636335401,\n",
       " 0.619115593545166,\n",
       " 0.6184199318044115,\n",
       " 0.6179274406001286,\n",
       " 0.6171416855615118,\n",
       " 0.6163007603375376,\n",
       " 0.6151647047280003,\n",
       " 0.6153540098886588,\n",
       " 0.6142606509292836,\n",
       " 0.6130643920658793,\n",
       " 0.61228400316512,\n",
       " 0.6117419562219507,\n",
       " 0.6112681848391872,\n",
       " 0.6107292598687174,\n",
       " 0.6091047005399167,\n",
       " 0.60929991046209,\n",
       " 0.6078371115774824,\n",
       " 0.6076314408868007,\n",
       " 0.6066912981355856,\n",
       " 0.6059501603719296,\n",
       " 0.6055330689827128,\n",
       " 0.6045187624734512,\n",
       " 0.6042159948170994,\n",
       " 0.6031508229825107,\n",
       " 0.6027975345046463,\n",
       " 0.601634985371198,\n",
       " 0.6013000395325501,\n",
       " 0.6002200743322282,\n",
       " 0.59880916772413,\n",
       " 0.598481970765829,\n",
       " 0.5985718560781941,\n",
       " 0.5976133353127285,\n",
       " 0.5962353005991775,\n",
       " 0.5962538076345774,\n",
       " 0.5948216994344169,\n",
       " 0.5944064050369726,\n",
       " 0.5935887077935884,\n",
       " 0.592994883650324,\n",
       " 0.5922468154769328,\n",
       " 0.5913856448802163,\n",
       " 0.5909220777387333,\n",
       " 0.5902635975729691,\n",
       " 0.5902010213396051,\n",
       " 0.5885833503854173,\n",
       " 0.5878154421044707,\n",
       " 0.5874772580006073,\n",
       " 0.5862353975614869,\n",
       " 0.5858489744254285,\n",
       " 0.5857943741748829,\n",
       " 0.5846859488380769,\n",
       " 0.5845222924581533,\n",
       " 0.5835925650789227,\n",
       " 0.58283461671215,\n",
       " 0.5816717848509574,\n",
       " 0.5813499885285807,\n",
       " 0.580508966786575,\n",
       " 0.5800557710715476,\n",
       " 0.5792593018943146,\n",
       " 0.5787461264520186,\n",
       " 0.578208499238806,\n",
       " 0.5778779494270916,\n",
       " 0.5765112403914343,\n",
       " 0.5762030418515545,\n",
       " 0.5754707412156651,\n",
       " 0.5747539590110516,\n",
       " 0.5738821892992172,\n",
       " 0.5730566336486878,\n",
       " 0.5723996889276581,\n",
       " 0.5724113187799976,\n",
       " 0.5710633332370907,\n",
       " 0.5702056393310374,\n",
       " 0.570567204045819,\n",
       " 0.5688183735921376,\n",
       " 0.5690669747562936,\n",
       " 0.5677307570021255,\n",
       " 0.5670504156633692,\n",
       " 0.5671147181291027,\n",
       " 0.5655241464277257,\n",
       " 0.5652706720642521,\n",
       " 0.5644564125725076,\n",
       " 0.5643874091473463,\n",
       " 0.5637358103420999,\n",
       " 0.5628268421625339,\n",
       " 0.5629981775612968,\n",
       " 0.5612490279074984,\n",
       " 0.5610755708732226,\n",
       " 0.560156464131459,\n",
       " 0.5591496142322253,\n",
       " 0.5592400457611433,\n",
       " 0.5581356330216102,\n",
       " 0.5577240743810964,\n",
       " 0.5569976565238891,\n",
       " 0.5571917081841974,\n",
       " 0.555678110242551,\n",
       " 0.5550168963674691,\n",
       " 0.554533189865443,\n",
       " 0.5540211533053777,\n",
       " 0.5536436728201942,\n",
       " 0.5525330666343231,\n",
       " 0.5522129658700944,\n",
       " 0.551372525183146,\n",
       " 0.5513191683907818,\n",
       " 0.5498098165737473,\n",
       " 0.5491745840038811,\n",
       " 0.5489377282525117,\n",
       " 0.5481613550268539,\n",
       " 0.5477915021284344,\n",
       " 0.5473607640875102,\n",
       " 0.5463849566315517,\n",
       " 0.5458245964564411,\n",
       " 0.5450582877964351,\n",
       " 0.5441351622776781,\n",
       " 0.5430303987065545,\n",
       " 0.5432362268988453,\n",
       " 0.542812269243532,\n",
       " 0.5421869531183462,\n",
       " 0.5415730010540527,\n",
       " 0.5406025208614392,\n",
       " 0.5403615273109881,\n",
       " 0.5388617849832282,\n",
       " 0.5387583992255448,\n",
       " 0.53900532841522,\n",
       " 0.5378037394666245,\n",
       " 0.5369332334337651,\n",
       " 0.5371343581424312,\n",
       " 0.5359520953175457,\n",
       " 0.5357344451985258,\n",
       " 0.5347202013608412,\n",
       " 0.5340732774857301,\n",
       " 0.533434417666083,\n",
       " 0.5333508346892716,\n",
       " 0.5325614757064266,\n",
       " 0.531949891620909,\n",
       " 0.5309734719409452,\n",
       " 0.5306740432237198,\n",
       " 0.5301274032425869,\n",
       " 0.529093382077076,\n",
       " 0.5293119520226234,\n",
       " 0.5285955364719304,\n",
       " 0.5278448644981121,\n",
       " 0.5268075166328208,\n",
       " 0.5262435509728054,\n",
       " 0.5260120130581762,\n",
       " 0.5245580594040515,\n",
       " 0.5246507042067915,\n",
       " 0.5240877520846384,\n",
       " 0.5241449947318342,\n",
       " 0.5228906732941956,\n",
       " 0.5224058812197331,\n",
       " 0.5214831715128649,\n",
       " 0.5212350164604094,\n",
       " 0.5203918665256106,\n",
       " 0.5201414869837508,\n",
       " 0.5195856677958522,\n",
       " 0.5181638970986526,\n",
       " 0.5180629479229588,\n",
       " 0.5174336543023949,\n",
       " 0.5172290933447917,\n",
       " 0.5168114765148318,\n",
       " 0.5164558743197049,\n",
       " 0.5158948597839162,\n",
       " 0.5152535628464161,\n",
       " 0.5139800660069846,\n",
       " 0.5138535658179171,\n",
       " 0.5126843586317988,\n",
       " 0.5128742557883191,\n",
       " 0.512179084611097,\n",
       " 0.5120866623550361,\n",
       " 0.5115323733376105,\n",
       " 0.5105870657536816,\n",
       " 0.509788943645313,\n",
       " 0.5101774275777268,\n",
       " 0.5088662840683418,\n",
       " 0.508238627934242,\n",
       " 0.5077854918271483,\n",
       " 0.5080223251710564,\n",
       " 0.5071881404296343,\n",
       " ...]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = len(np.unique(y_train))\n",
    "\n",
    "numpy_model = NumpyNN(input_dim, hidden_dim, output_dim, lr=0.001)\n",
    "numpy_model.train(X_train, y_train, epochs=3500, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d94f19ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NumPy NN Accuracy: 0.9091703056768559\n",
      "\n",
      "Classification report (NumPy NN):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.92      0.90      0.91       942\n",
      "           2       0.91      0.91      0.91      1269\n",
      "           3       0.90      0.92      0.91      1040\n",
      "           4       0.89      0.92      0.91       687\n",
      "           5       0.90      0.90      0.90       527\n",
      "           6       0.94      0.86      0.90       115\n",
      "\n",
      "    accuracy                           0.91      4580\n",
      "   macro avg       0.91      0.90      0.91      4580\n",
      "weighted avg       0.91      0.91      0.91      4580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_np = numpy_model.predict(X_test)\n",
    "acc_np = accuracy_score(y_test, y_pred_np)\n",
    "print(\"\\nNumPy NN Accuracy:\", acc_np)\n",
    "print(\"\\nClassification report (NumPy NN):\\n\", classification_report(y_test, y_pred_np, target_names=target_names1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
